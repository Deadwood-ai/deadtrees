# Tree Cover Segmentation Implementation Specification

## Overview

This specification details the implementation of tree cover segmentation using a **hybrid approach** that combines the official TCD Docker container with proven custom processing logic. The solution uses the `ghcr.io/restor-foundation/tcd:main` container for ML inference while preserving the working implementation from the treecover-segmentation branch for parameter handling and result processing.

## Architecture Pattern

**Hybrid Strategy**: Official TCD container for inference + Custom logic for processing  
**Container**: Use external `ghcr.io/restor-foundation/tcd:main` container from [Restor Foundation TCD repo](https://github.com/Restor-Foundation/tcd)  
**File Transfer**: Shared Docker volumes via `shared_volume.py` utilities  
**Processing Logic**: Preserve proven implementation for preprocessing, parameter handling, and polygon conversion  
**Database Integration**: Store results in `v2_forest_cover_geometries` via existing labels system  
**Processing Order**: Runs after deadwood segmentation in the standard pipeline  

## Technical Requirements

### Dependencies
- **Docker container**: `ghcr.io/restor-foundation/tcd:main` (official TCD container)
- **Original processing logic**: From `tree_cover_inference.py` (treecover-segmentation branch)
- **Existing utilities**: Shared volume utilities (`processor/src/utils/shared_volume.py`)
- **Database**: Forest cover geometry tables (`v2_forest_cover_geometries`)
- **Integration**: Labels system (`shared/labels.py`) and existing `mask_to_polygons()` utility

### Processing Pipeline Integration
```
ODM → GeoTIFF → COG → Thumbnail → Metadata → Deadwood → TreeCover
```

### Hybrid Processing Flow
1. **Preprocessing**: Use original logic for image reprojection and parameter setup
2. **Inference**: Execute TCD container via shared volumes (replaces Python API calls)
3. **Postprocessing**: Use original logic for confidence map processing and polygon conversion
4. **Storage**: Use existing labels system for database integration

### Input/Output Specifications
- **Input**: Orthomosaic file from `ortho_file_name` (same as deadwood segmentation)
- **Preprocessing**: Reproject to EPSG:3395, resample to 10cm resolution (original implementation)
- **Container Processing**: TCD semantic segmentation via Docker container
- **Postprocessing**: Confidence thresholding (200), binary mask creation, polygon conversion
- **Output**: Polygons stored in `v2_forest_cover_geometries` table via labels system
- **File Cleanup**: Temporary containers and volumes cleaned up after processing

## Implementation Tasks

### Phase 1: Core Infrastructure Setup
- [ ] 1.1 Add `treecover` to `TaskTypeEnum` in `shared/models.py`
- [ ] 1.2 Add `LogCategory.TREECOVER` to logging system
- [ ] 1.3 Update status tracking with `is_treecover_done` field if needed
- [ ] 1.4 Add treecover processing integration to `processor/src/processor.py`

### Phase 2: Hybrid Processing Logic Implementation
- [ ] 2.1 Replace `convert_to_projected()` with `rasterio.warp.reproject()` for EPSG:3395 + 10cm resampling
- [ ] 2.2 Create `copy_ortho_to_tcd_volume()` function with reprojected orthomosaic
- [ ] 2.3 Create `copy_tcd_results_from_volume()` function for confidence map extraction
- [ ] 2.4 Implement `_load_confidence_map_from_container_output()` to read container's `confidence_map.tif`
- [ ] 2.5 Implement `_run_tcd_container()` following ODM container pattern
- [ ] 2.6 Add TCD-specific volume cleanup and error handling

### Phase 3: Core Processing Integration
- [ ] 3.1 Create `processor/src/process_treecover_segmentation.py` main processing function
- [ ] 3.2 Implement authentication and ortho file retrieval (follow deadwood pattern)
- [ ] 3.3 Add file download from storage server via SSH
- [ ] 3.4 Integrate preprocessing → TCD container → postprocessing workflow
- [ ] 3.5 Replace `tcd_pipeline.pipeline.Pipeline` calls with container execution

### Phase 4: Result Processing with Original Logic
- [ ] 4.1 Create `processor/src/treecover_segmentation/predict_treecover.py` for result parsing
- [ ] 4.2 Preserve original confidence map handling (DatasetReader vs numpy array detection)
- [ ] 4.3 Implement original thresholding logic: `(confidence_map > 200).astype(np.uint8)`
- [ ] 4.4 Use existing `mask_to_polygons()` utility from common module
- [ ] 4.5 Add coordinate reprojection from EPSG:3395 back to WGS84 for database storage
- [ ] 4.6 Integrate with labels system for `v2_forest_cover_geometries` storage

### Phase 5: Testing Implementation
- [ ] 5.1 Create `processor/tests/test_process_treecover_segmentation.py` with basic functionality test
- [ ] 5.2 Add TCD container integration test (pull container, run prediction)
- [ ] 5.3 Test database storage of forest cover geometries
- [ ] 5.4 Verify complete pipeline integration (deadwood → treecover processing)

### Phase 6: Documentation and Cleanup
- [ ] 6.1 Add error handling and logging throughout the pipeline
- [ ] 6.2 Update processor documentation with treecover task type
- [ ] 6.3 Verify container and volume cleanup in all error scenarios
- [ ] 6.4 Final integration testing with existing task queue system

## Detailed Implementation Notes

### Hybrid Workflow Architecture
```python
# 1. Preprocessing: Reproject orthomosaic for container input
rasterio.warp.reproject(input_tif, temp_reproject_path, dst_crs="EPSG:3395", resolution=0.1)

# 2. Container execution: TCD inference via Docker
_run_tcd_container(temp_reproject_path, output_dir, dataset_id, token)

# 3. Postprocessing: Load results and convert to polygons  
confidence_map = _load_confidence_map_from_container_output(output_dir)
outimage = (confidence_map > TCD_THRESHOLD).astype(np.uint8)
polygons = mask_to_polygons(outimage, dataset)
```

### Container Command Structure
Based on [TCD documentation](https://github.com/Restor-Foundation/tcd), the container will use:
```bash
docker run ghcr.io/restor-foundation/tcd:main \
  tcd-predict semantic \
  /tcd_data/dataset_{dataset_id}/input/orthomosaic_reprojected.tif \
  /tcd_data/dataset_{dataset_id}/output \
  --model=restor/tcd-segformer-mit-b5
```

### File Organization in Shared Volume
```
/tcd_data/                              # Mounted shared volume
├── dataset_{dataset_id}/
│   ├── input/
│   │   └── orthomosaic_reprojected.tif  # EPSG:3395, 10cm resolution
│   └── output/
│       ├── confidence_map.tif           # TCD container output (load this)
│       ├── predictions.shp             # Container predictions (ignore)
│       └── overlays/                   # Container visualizations (ignore)
```

### Original Implementation Preservation
Key elements preserved from `tree_cover_inference.py`:
```python
# Image preprocessing - replace convert_to_projected() with rasterio
# (exact same output: EPSG:3395, 10cm resolution)
with rasterio.open(input_tif) as src:
    transform, width, height = rasterio.warp.calculate_default_transform(
        src.crs, 'EPSG:3395', src.width, src.height, *src.bounds, resolution=0.1)
    
    with rasterio.open(temp_reproject_path, 'w', **profile) as dst:
        rasterio.warp.reproject(...)

# Container output loading - NEW bridge logic
def _load_confidence_map_from_container_output(output_dir: Path) -> np.ndarray:
    """Load confidence map from TCD container output"""
    confidence_tif = output_dir / 'confidence_map.tif'
    with rasterio.open(confidence_tif) as src:
        return src.read(1)  # Read first band as numpy array

# Confidence map handling (simplified - always numpy array from file)
confidence_map = _load_confidence_map_from_container_output(output_dir)

# Thresholding and polygon conversion (original logic - unchanged)
TCD_THRESHOLD = 200
outimage = (confidence_map > TCD_THRESHOLD).astype(np.uint8)
polygons = mask_to_polygons(outimage, dataset)  # Existing utility
```

### Database Schema Usage
Utilize existing `v2_forest_cover_geometries` table:
- `label_id`: Links to `v2_labels` table
- `geometry`: MultiPolygon in WGS84 (EPSG:4326) - reprojected from EPSG:3395
- `properties`: JSON metadata (confidence scores, processing parameters, original config)

### Error Handling Strategy
- Container creation/execution failures
- TCD container processing errors (no trees detected, invalid input)
- File transfer failures between shared volumes
- Original preprocessing failures (reprojection, resampling)
- Confidence map loading and processing errors
- Database storage errors
- Proper cleanup of Docker resources in all scenarios

### Configuration Parameters (Original Implementation)
- **Tree Cover Threshold**: 200 (from `treecover_inference_config.json`)
- **Model**: `"restor/tcd-segformer-mit-b5"` (proven working model)
- **Target Resolution**: 0.1m (10cm - from original config)
- **Projection**: EPSG:3395 (Mercator - for processing)
- **Output Projection**: EPSG:4326 (WGS84 - for database storage)
- **Processing Resolution**: `target_gsd_m=0.1` (original parameter)

### Testing Strategy Philosophy
- **Real Container Testing**: Use actual `ghcr.io/restor-foundation/tcd:main` container
- **Real Database**: Test against actual Supabase with real geometry storage
- **Minimal Test Coverage**: Focus on core functionality (container execution, result storage)
- **Integration Testing**: Verify deadwood → treecover task sequence works correctly

### Performance Considerations
- TCD processing is typically faster than ODM, no dev/prod separation needed
- Container startup time should be minimal with pre-pulled images
- Shared volume approach eliminates host path dependencies
- Memory cleanup after processing to handle large orthomosaics

### Dependency Management
- **TCD Container**: Official `ghcr.io/restor-foundation/tcd:main` handles ML model dependencies
- **Preprocessing Solution**: Replace `convert_to_projected()` with GDAL/rasterio (already available):
  ```python
  # Replace TCD convert_to_projected() with:
  rasterio.warp.reproject() + rasterio.warp.calculate_default_transform()
  # Target: EPSG:3395, 0.1m resolution (exact same output as original)
  ```
- **Existing Dependencies**: Docker Python SDK, SSH utilities, labels system, rasterio, numpy
- **No Additional Dependencies**: Avoid TCD Python package entirely - use containerized inference only

## Reference Implementation Patterns

**Follow ODM Pattern** (`process_odm.py`):
- Docker container creation and execution
- Shared volume file transfer  
- Error handling and cleanup
- Logging integration

**Follow Deadwood Pattern** (`process_deadwood_segmentation.py`):
- Authentication and file retrieval
- Database integration via labels system
- Status updates and error reporting
- Result polygon processing

**Follow Original TreeCover Logic** (`tree_cover_inference.py`):
- Image preprocessing with `convert_to_projected()`
- Confidence map type detection and handling
- Threshold-based binary mask creation
- Polygon conversion using existing utilities

**File Transfer Pattern** (`shared_volume.py`):
- Named volume creation and cleanup
- Container-based file copying
- Tar archive handling for efficiency
- Proper Docker resource management

**Hybrid Integration Strategy**:
- Use container for inference (avoid dependency conflicts)
- Preserve proven preprocessing/postprocessing logic
- Maintain original parameters and thresholds
- Leverage existing utilities (`mask_to_polygons()`, labels system)

## Success Criteria

1. **Task Integration**: `TaskTypeEnum.treecover` works in processor queue system
2. **Container Execution**: TCD container runs successfully with shared volumes
3. **Result Storage**: Forest cover polygons stored correctly in database
4. **Pipeline Flow**: Deadwood → Treecover sequence executes without conflicts
5. **Resource Cleanup**: No leaked containers, volumes, or temporary files
6. **Error Recovery**: Graceful handling of container and processing failures

This specification provides a complete roadmap for implementing tree cover segmentation using the proven Docker container pattern established with ODM processing, while preserving the working implementation logic from the treecover-segmentation branch and integrating seamlessly with the existing deadwood segmentation pipeline.