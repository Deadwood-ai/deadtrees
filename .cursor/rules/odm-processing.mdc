---
description: Implementing or troubleshooting ODM-related functionality
alwaysApply: false
---
# ODM Processing

## WHEN TO USE
- Implementing or troubleshooting ODM-related functionality
- Running or inspecting `processor/src/process_odm.py`

## Requirements
- CPU-only (GPU not used for ODM with current Docker approach)
- Docker socket mounted into processor container (`/var/run/docker.sock`)
- Sufficient CPU/RAM reservations in compose (see `docker-compose.processor.yaml`)

## Key Files
- `processor/src/process_odm.py` - Main ODM orchestration
- `processor/src/utils/shared_volume.py` - Volume management and data transfer
- `docker-compose.processor.yaml` (mounts, env, network_mode host)

## Data Flow Pattern (Named Volumes)
1. Processor SSH pulls ZIP from storage server
2. Creates named Docker volume: `odm_processing_{dataset_id}`
3. Copies images to volume via temp Alpine container
4. Runs ODM container with volume mounted at `/odm_data`
5. ODM writes results (orthomosaic, etc.) to volume
6. **Extract results from volume via temp container** (tar streaming)
7. Push results to storage server via SSH
8. Cleanup volume and temp containers

**Why Named Volumes?**
- API and Processor run on DIFFERENT physical machines in production
- No shared filesystem - processor pulls/pushes via SSH
- Named volumes avoid permission issues (UID conflicts with bind mounts)
- Test environment simulates this separation

## Inputs & Outputs
- Inputs: Dataset ID, SSH pulls imagery from storage server
- Outputs: Orthomosaic, COG, thumbnails pushed back to storage
- Temp work: Named volumes (not /data - processor doesn't have /data)

## Common Issues
- **Memory exhaustion during extraction** (tar streaming bug - see below)
- Zombie extract containers (Alpine containers stuck running)
- Permissions on Docker socket (inside container)
- Path resolution: always use `settings.*_path` helpers

## Known Bug: Tar Extraction Memory Issue
**Location:** `processor/src/utils/shared_volume.py:189`

**Symptom:** 
- Extraction hangs forever on large datasets (>10GB)
- No error messages
- Extract container runs indefinitely with `tail -f /dev/null`
- Memory exhaustion or OOM

**Root Cause:**
```python
# BROKEN - loads entire 40GB archive into RAM!
with tarfile.open(mode='r|', fileobj=io.BytesIO(b''.join(archive_stream))) as tar:
    tar.extractall(output_dir)
```

**Fix:**
```python
# FIXED - streams without loading into memory
with tarfile.open(mode='r|*', fileobj=archive_stream) as tar:
    for member in tar:
        tar.extract(member, output_dir)
        # Log progress every 100 files
```

## Debugging
- Use `deadtrees dev test processor --test-path=processor/tests/test_process_odm.py` for regular runs.
- Use `deadtrees dev debug processor --test-path=processor/tests/test_process_odm.py` only when you need breakpoints (port 5678 must be free).
- Check logs via `shared.logger` and processor container output.
- Reproduce failures with real datasets by staging ZIPs locally and pointing tests to them:

  1) Create a debug folder and copy the raw ZIP from storage:
     - `mkdir -p assets/test_data/debugging/<dataset_id>`
     - `scp -i ~/.ssh/processing-to-storage dendro@data2.deadtrees.earth:/data/raw_images/<dataset_id>.zip assets/test_data/debugging/<dataset_id>/`

  2) Optionally copy the current ortho for inspection:
     - `scp -i ~/.ssh/processing-to-storage dendro@data2.deadtrees.earth:/data/archive/<dataset_id>_ortho.tif assets/test_data/debugging/<dataset_id>/`

  3) Run the existing ODM test against the debug ZIP. The fixture resolves ZIPs in this order:
     - `DEBUG_ODM_ZIP` env var (absolute path to a local ZIP)
     - Any ZIP under `assets/test_data/debugging/`
     - Default minimal 5-image dataset

     Examples:
     - Auto-pick from debugging folder:
       `deadtrees dev test processor --test-path=processor/tests/test_process_odm.py::test_complete_odm_processing_with_real_images`
     - Target a specific ZIP explicitly:
       `DEBUG_ODM_ZIP=$(pwd)/assets/test_data/debugging/5386/5386.zip deadtrees dev test processor --test-path=processor/tests/test_process_odm.py::test_complete_odm_processing_with_real_images`

  4) Behavior: with debug ZIPs the test asserts a proper error is recorded in `v2_statuses`; with the default test ZIP it expects full success and metadata updates.

## Notes
- Ask for missing context (e.g., dataset structure) instead of assuming
- See `deployment-architecture.mdc` for full architecture context
