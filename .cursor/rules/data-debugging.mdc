---
description: - Investigating failed or problematic datasets (like dataset 3904) - Debugging data processing pipeline issues - Using MCP tools to examine production database - Analyzing dataset status, errors, or processing failures
alwaysApply: false
---
# Data Debugging & Dataset Investigation

## ðŸ“‹ **WHEN TO USE THIS RULE**
**Agent should request this rule when:**
- Investigating failed or problematic datasets (like dataset 3904)
- Debugging data processing pipeline issues
- Using MCP tools to examine production database
- Analyzing dataset status, errors, or processing failures

## ðŸŽ¯ **CORE PRINCIPLE**
**ALWAYS use MCP tools for production database investigation. NEVER use deadtrees CLI for database connections.**

## âš¡ **QUICK START: Essential Investigation Query**
```sql
-- Run this FIRST for any dataset issue
SELECT 
  d.id, d.file_name, d.created_at,
  s.current_status, s.has_error, s.error_message,
  s.is_upload_done, s.is_ortho_done, s.is_cog_done, 
  s.is_thumbnail_done, s.is_deadwood_done, s.is_forest_cover_done
FROM v2_datasets d 
JOIN v2_statuses s ON d.id = s.dataset_id 
WHERE d.id = ?;
```

## ðŸ”§ **MCP INVESTIGATION TOOLS**
```bash
# Essential MCP commands
mcp_deadtrees-prod_execute_sql           # Query database
mcp_deadtrees-prod_get_object_details    # Table schema
mcp_deadtrees-prod_explain_query         # Performance analysis
mcp_deadtrees-prod_analyze_db_health     # System health
```

## ðŸš¨ **COMMON ISSUE PATTERNS**

### File Corruption (Dataset 3904, Issue #194)
**Symptoms:** Tiny file sizes vs large dimensions, `has_error: true` with `error_message: null`
```sql
-- Check file size anomalies
SELECT dataset_id, ortho_file_size,
  ortho_info->'Profile'->>'Width' as width,
  ortho_info->'Profile'->>'Height' as height,
  ortho_file_size::float / ((ortho_info->'Profile'->>'Width')::int * (ortho_info->'Profile'->>'Height')::int) as bytes_per_pixel
FROM v2_orthos WHERE dataset_id = ?;
```

### Missing CRS (Issue #184)
**Symptoms:** "No CRS found" errors, stuck in `ortho_processing`
```sql
-- Check CRS information
SELECT dataset_id, ortho_info->'GEO'->>'CRS' as crs
FROM v2_orthos WHERE dataset_id = ?;
```

### Segmentation Failures (Issue #161)
**Symptoms:** `_TIFFPartialReadStripArray` errors, `IReadBlock failed`
```sql
-- Check segmentation results
SELECT l.id, l.dataset_id, COUNT(dg.id) as geometry_count
FROM v2_labels l 
LEFT JOIN v2_deadwood_geometries dg ON l.id = dg.label_id 
WHERE l.dataset_id = ? GROUP BY l.id;
```

## ðŸ“ **6-STEP INVESTIGATION WORKFLOW**

### 1. Dataset Overview
```sql
SELECT d.*, s.current_status, s.has_error, s.error_message
FROM v2_datasets d JOIN v2_statuses s ON d.id = s.dataset_id 
WHERE d.id = ?;
```

### 2. File Analysis
```sql
SELECT dataset_id, ortho_file_size, ortho_info->'COG_errors' as errors
FROM v2_orthos WHERE dataset_id = ?;
```

### 3. Processing Results
```sql
SELECT dataset_id, cog_file_size, thumbnail_file_size
FROM v2_cogs c JOIN v2_thumbnails t USING(dataset_id) 
WHERE dataset_id = ?;
```

### 4. Error Analysis
```sql
SELECT level, message, created_at, category
FROM v2_logs WHERE dataset_id = ? AND level = 'ERROR'
ORDER BY created_at DESC LIMIT 10;
```

### 5. Server Resource Checks (Host)
```sql
-- Derive processing time window from logs (e.g., ODM)
WITH w AS (
  SELECT date_trunc('minute', MIN(created_at)) AS t_start,
         date_trunc('minute', MAX(created_at)) AS t_end
  FROM v2_logs
  WHERE dataset_id = ? AND category = 'odm'
)
SELECT * FROM w;
```

```bash
# On Ubuntu hosts with sysstat enabled (replace DD with day-of-month, times from query)
# CPU
LC_ALL=C sar -u -f /var/log/sysstat/saDD -s HH:MM:SS -e HH:MM:SS

# Memory and swap
LC_ALL=C sar -r -f /var/log/sysstat/saDD -s HH:MM:SS -e HH:MM:SS
LC_ALL=C sar -S -f /var/log/sysstat/saDD -s HH:MM:SS -e HH:MM:SS

# Load averages / run queue
LC_ALL=C sar -q -f /var/log/sysstat/saDD -s HH:MM:SS -e HH:MM:SS

# Kernel OOM / kill events in window
journalctl -k --since 'YYYY-MM-DD HH:MM:SS' --until 'YYYY-MM-DD HH:MM:SS' | egrep -i 'oom|killed process|out of memory'

# (Optional) Docker daemon events in window
docker events --since 'YYYY-MM-DDTHH:MM:SS' --until 'YYYY-MM-DDTHH:MM:SS'
```

Interpretation checklist:
- CPU: sustained >85% busy or high iowait (>5%) may indicate contention/IO bottlenecks.
- RAM: %memused >90% or rising swap >10% suggests memory pressure; check journalctl for OOM.
- Load: compare load vs CPU cores; load >> cores with low CPU% may be IO-bound.
- If host looks healthy, prefer data/content or container-level root causes.

### 6. System Health
```bash
# Use MCP tool
mcp_deadtrees-prod_analyze_db_health: health_type="all"
```

## ðŸ”§ **EMERGENCY PROCEDURES**
```sql
-- Reset stuck dataset (with proper authorization)
UPDATE v2_statuses 
SET current_status = 'idle', has_error = false 
WHERE dataset_id = ? AND current_status != 'idle';

-- Remove from processing queue
DELETE FROM v2_queue WHERE dataset_id = ?;
```

## ðŸ“Š **MONITORING QUERIES**
```sql
-- Find stuck processing (> 1 hour)
SELECT d.id, d.file_name, s.current_status, 
  EXTRACT(EPOCH FROM (NOW() - s.updated_at))/3600 as hours_stuck
FROM v2_datasets d JOIN v2_statuses s ON d.id = s.dataset_id 
WHERE s.current_status != 'idle' 
AND s.updated_at < NOW() - INTERVAL '1 hour';

-- Error rate summary
SELECT 
  COUNT(*) FILTER (WHERE has_error = true) as error_count,
  COUNT(*) as total_count,
  ROUND(100.0 * COUNT(*) FILTER (WHERE has_error = true) / COUNT(*), 2) as error_rate
FROM v2_statuses s JOIN v2_datasets d ON s.dataset_id = d.id
WHERE d.created_at > NOW() - INTERVAL '24 hours';
``` 

## ðŸ§  Improvements & Additional Data to Capture

- Persist host metrics snapshots per dataset/time window in DB (new table proposal, e.g., `v2_host_metrics`):
  - dataset_id, t_start, t_end, avg_cpu_user, avg_cpu_system, avg_cpu_iowait, avg_mem_used_pct,
    avg_swap_used_mb, avg_load1, avg_load5, avg_load15
- Capture container-level stats and exit reasons:
  - docker exit code, last N lines of stdout/stderr, container OOM (from cgroup/journal), image tag
- Log processing parameters for reproducibility:
  - ODM command/flags (resolution, feature quality, neighbors), environment (DEV/PROD)
- Enrich raw data inventory:
  - image_count (done), total_size_bytes (done), per-extension counts, size histogram, skipped/corrupt files list
- EXIF summary:
  - first/last `DateTimeOriginal`, camera `Make/Model`, focal length range, GPS availability ratio
- Stage timing breakdown:
  - extract zip, RTK detection, EXIF extraction, ODM stages (opensfm, mve/meshing, orthophoto), COG, thumbnail
- Storage/IO telemetry:
  - SSH/S3 transfer durations, retries, bandwidth; disk free space checks before processing
- Error taxonomy & remediation hints:
  - classify frequent failures (CRS missing, corrupt TIFF, projection mismatch, insufficient overlap) and suggest fixes

Operational tips to speed triage:
- Always derive the exact processing window from `v2_logs` and correlate with sysstat/journalctl.
- Keep small helper scripts to map dataset timestamp â†’ `saDD` file and run `sar` commands automatically.
- Consider writing a CLI helper that outputs a compact resource summary next to DB log excerpts for any dataset.