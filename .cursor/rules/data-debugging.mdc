---
description: Investigating failed or problematic datasets (like dataset 3904) - Debugging data processing pipeline issues - Using MCP tools to examine production database - Analyzing dataset status, errors, or processing failures - Rerunning datasets after fixing issues
alwaysApply: false
---
# Data Debugging & Dataset Investigation

## üìã **WHEN TO USE THIS RULE**
**Agent should request this rule when:**
- Investigating failed or problematic datasets (like dataset 3904)
- Debugging data processing pipeline issues
- Using MCP tools to examine production database
- Analyzing dataset status, errors, or processing failures
- Rerunning datasets after identifying and fixing root causes

## üéØ **CORE PRINCIPLE**
**ALWAYS use MCP tools for production database investigation. NEVER use deadtrees CLI for database connections.**

## üîÑ **PROCESSING ORDER** (see `processor-pipeline.mdc` for full details)

Tasks execute in this **strict order**:

1. **ODM** (if raw drone images) ‚Üí Generates ortho from raw images
2. **GeoTIFF Standardization** ‚Üí Standardizes/tiles ortho (**stays LOCAL in processor**, not pushed back)
3. **Metadata** ‚Üí GADM, biome, phenology extraction
4. **COG** ‚Üí Cloud-optimized GeoTIFF generation
5. **Thumbnail** ‚Üí Preview image generation
6. **Deadwood** ‚Üí Deadwood segmentation
7. **Tree Cover** ‚Üí Forest cover segmentation

**CRITICAL: Standardized File Flow**
- Original ortho: `/data/archive/XXXX_ortho.tif` (preserved, may be untiled)
- Standardized ortho: Processor temp dir ONLY (tiled, not pushed back to storage)
- All downstream tasks use the **local standardized file** (COG, thumbnail, segmentation)
- Metadata stored in `v2_orthos_processed` table
- This is why `process_geotiff.py` line 195 is commented out (intentional!)

## ‚ö° **QUICK START: Essential Investigation Query**
```sql
-- Run this FIRST for any dataset issue
SELECT 
  d.id, d.file_name, d.created_at,
  s.current_status, s.has_error, s.error_message,
  s.is_upload_done, s.is_ortho_done, s.is_cog_done, 
  s.is_thumbnail_done, s.is_deadwood_done, s.is_forest_cover_done
FROM v2_datasets d 
JOIN v2_statuses s ON d.id = s.dataset_id 
WHERE d.id = ?;
```

## üîß **MCP INVESTIGATION TOOLS**
```bash
# Essential MCP commands
mcp_deadtrees-prod_execute_sql           # Query database
mcp_deadtrees-prod_get_object_details    # Table schema
mcp_deadtrees-prod_explain_query         # Performance analysis
mcp_deadtrees-prod_analyze_db_health     # System health
```

## üö® **COMMON ISSUE PATTERNS**

### File Corruption (Dataset 3904, Issue #194)
**Symptoms:** Tiny file sizes vs large dimensions, `has_error: true` with `error_message: null`
```sql
-- Check file size anomalies
SELECT dataset_id, ortho_file_size,
  ortho_info->'Profile'->>'Width' as width,
  ortho_info->'Profile'->>'Height' as height,
  ortho_file_size::float / ((ortho_info->'Profile'->>'Width')::int * (ortho_info->'Profile'->>'Height')::int) as bytes_per_pixel
FROM v2_orthos WHERE dataset_id = ?;
```

### Missing CRS (Issue #184)
**Symptoms:** "No CRS found" errors, stuck in `ortho_processing`
```sql
-- Check CRS information
SELECT dataset_id, ortho_info->'GEO'->>'CRS' as crs
FROM v2_orthos WHERE dataset_id = ?;
```

### Segmentation Failures (Issue #161)
**Symptoms:** `_TIFFPartialReadStripArray` errors, `IReadBlock failed`
```sql
-- Check segmentation results
SELECT l.id, l.dataset_id, COUNT(dg.id) as geometry_count
FROM v2_labels l 
LEFT JOIN v2_deadwood_geometries dg ON l.id = dg.label_id 
WHERE l.dataset_id = ? GROUP BY l.id;
```

## üìù **6-STEP INVESTIGATION WORKFLOW**

### 1. Dataset Overview
```sql
SELECT d.*, s.current_status, s.has_error, s.error_message
FROM v2_datasets d JOIN v2_statuses s ON d.id = s.dataset_id 
WHERE d.id = ?;
```

### 2. File Analysis
```sql
SELECT dataset_id, ortho_file_size, ortho_info->'COG_errors' as errors
FROM v2_orthos WHERE dataset_id = ?;
```

### 3. Processing Results
```sql
SELECT dataset_id, cog_file_size, thumbnail_file_size
FROM v2_cogs c JOIN v2_thumbnails t USING(dataset_id) 
WHERE dataset_id = ?;
```

### 4. Error Analysis
```sql
SELECT level, message, created_at, category
FROM v2_logs WHERE dataset_id = ? AND level = 'ERROR'
ORDER BY created_at DESC LIMIT 10;
```

### 5. Server Resource Checks (Host)
```sql
-- Derive processing time window from logs (e.g., ODM)
WITH w AS (
  SELECT date_trunc('minute', MIN(created_at)) AS t_start,
         date_trunc('minute', MAX(created_at)) AS t_end
  FROM v2_logs
  WHERE dataset_id = ? AND category = 'odm'
)
SELECT * FROM w;
```

```bash
# On Ubuntu hosts with sysstat enabled (replace DD with day-of-month, times from query)
# CPU
LC_ALL=C sar -u -f /var/log/sysstat/saDD -s HH:MM:SS -e HH:MM:SS

# Memory and swap
LC_ALL=C sar -r -f /var/log/sysstat/saDD -s HH:MM:SS -e HH:MM:SS
LC_ALL=C sar -S -f /var/log/sysstat/saDD -s HH:MM:SS -e HH:MM:SS

# Load averages / run queue
LC_ALL=C sar -q -f /var/log/sysstat/saDD -s HH:MM:SS -e HH:MM:SS

# Kernel OOM / kill events in window
journalctl -k --since 'YYYY-MM-DD HH:MM:SS' --until 'YYYY-MM-DD HH:MM:SS' | egrep -i 'oom|killed process|out of memory'

# (Optional) Docker daemon events in window
docker events --since 'YYYY-MM-DDTHH:MM:SS' --until 'YYYY-MM-DDTHH:MM:SS'
```

Interpretation checklist:
- CPU: sustained >85% busy or high iowait (>5%) may indicate contention/IO bottlenecks.
- RAM: %memused >90% or rising swap >10% suggests memory pressure; check journalctl for OOM.
- Load: compare load vs CPU cores; load >> cores with low CPU% may be IO-bound.
- If host looks healthy, prefer data/content or container-level root causes.

### 6. System Health
```bash
# Use MCP tool
mcp_deadtrees-prod_analyze_db_health: health_type="all"
```

## üîß **EMERGENCY PROCEDURES**
```sql
-- Reset stuck dataset (with proper authorization)
UPDATE v2_statuses 
SET current_status = 'idle', has_error = false 
WHERE dataset_id = ? AND current_status != 'idle';

-- Remove from processing queue
DELETE FROM v2_queue WHERE dataset_id = ?;
```

## üîÑ **RERUNNING DATASETS AFTER DEBUGGING**

### Preferred Method: SQL via Supabase Studio (Two Steps)

The API endpoint (`PUT /datasets/{id}/process`) only adds a queue entry ‚Äî it does **NOT** clear `has_error` on `v2_statuses`. The processor checks `has_error` and **drops tasks for errored datasets**. Therefore, always use this two-step SQL approach:

**Step 1 ‚Äî Clear error state:**
```sql
UPDATE v2_statuses 
SET has_error = false, 
    error_message = null, 
    current_status = 'idle',
    is_odm_done = false        -- reset the failed stage flag(s)
WHERE dataset_id = <DATASET_ID>;
```

Adjust which `is_*_done` flags to reset based on which stages need rerunning:
- `is_odm_done = false` ‚Äî ODM failed
- `is_ortho_done = false` ‚Äî GeoTIFF standardization failed
- `is_cog_done = false` ‚Äî COG generation failed
- `is_deadwood_done = false` ‚Äî Deadwood segmentation failed
- `is_forest_cover_done = false` ‚Äî Treecover segmentation failed

**Step 2 ‚Äî Clean up stale queue entries and insert new one:**
```sql
DELETE FROM v2_queue WHERE dataset_id = <DATASET_ID>;

INSERT INTO v2_queue (dataset_id, user_id, priority, task_types)
SELECT 
    <DATASET_ID>,
    d.user_id,
    1,  -- priority: 1=highest, 5=lowest
    ARRAY['odm_processing','geotiff','cog','thumbnail','metadata','deadwood','treecover']
FROM v2_datasets d
WHERE d.id = <DATASET_ID>;
```

The `user_id` is pulled automatically from `v2_datasets`.

**Task type enum values** (must use these exact strings in the array):
- `odm_processing` ‚Äî OpenDroneMap (raw images ‚Üí ortho)
- `geotiff` ‚Äî GeoTIFF standardization
- `cog` ‚Äî Cloud Optimized GeoTIFF
- `thumbnail` ‚Äî Thumbnail generation
- `metadata` ‚Äî Extract metadata (GADM, biome, phenology)
- `deadwood` ‚Äî Deadwood segmentation
- `treecover` ‚Äî Tree cover segmentation

**Common Rerun Scenarios:**

```sql
-- Full rerun from ODM (raw image dataset)
ARRAY['odm_processing','geotiff','cog','thumbnail','metadata','deadwood','treecover']

-- Rerun post-ODM only (ODM succeeded, downstream failed)
-- CRITICAL: always include geotiff ‚Äî standardized file is ephemeral
ARRAY['geotiff','cog','thumbnail','metadata','deadwood','treecover']

-- Rerun segmentation only (COG/thumbnail OK)
ARRAY['geotiff','deadwood','treecover']

-- Rerun just COG + thumbnail
ARRAY['geotiff','cog','thumbnail']
```

‚ö†Ô∏è **CRITICAL: Always include `geotiff`** when rerunning any downstream task. The standardized ortho only exists during a pipeline run ‚Äî it is NOT stored permanently. Without `geotiff`, downstream tasks run against the raw unstandardized file and fail or produce wrong results.

### Alternative: API Endpoint

The `PUT /api/v1/datasets/{id}/process` endpoint automatically clears `has_error` and resets the relevant `is_*_done` flags when re-queuing a failed dataset. Use it when you have API access:

```bash
# Authenticate against production Supabase
cd /home/jj1049/prod/deadtrees && source .env
TOKEN=$(curl -s -X POST "${SUPABASE_URL}/auth/v1/token?grant_type=password" \
  -H "apikey: ${SUPABASE_KEY}" \
  -H "Content-Type: application/json" \
  -d "{\"email\": \"processor@deadtrees.earth\", \"password\": \"${PROCESSOR_PASSWORD}\"}" \
  | python3 -c "import sys,json; print(json.load(sys.stdin)['access_token'])")

# Re-queue dataset
curl -X PUT "https://data2.deadtrees.earth/api/v1/datasets/<DATASET_ID>/process" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"task_types": ["odm_processing","geotiff","cog","thumbnail","metadata","deadwood","treecover"], "priority": 1}'
```

The SQL approach above is the fallback when you don't have API access or need finer control over which flags to reset.

## üìä **MONITORING QUERIES**
```sql
-- Find stuck processing (> 1 hour)
SELECT d.id, d.file_name, s.current_status, 
  EXTRACT(EPOCH FROM (NOW() - s.updated_at))/3600 as hours_stuck
FROM v2_datasets d JOIN v2_statuses s ON d.id = s.dataset_id 
WHERE s.current_status != 'idle' 
AND s.updated_at < NOW() - INTERVAL '1 hour';

-- Error rate summary
SELECT 
  COUNT(*) FILTER (WHERE has_error = true) as error_count,
  COUNT(*) as total_count,
  ROUND(100.0 * COUNT(*) FILTER (WHERE has_error = true) / COUNT(*), 2) as error_rate
FROM v2_statuses s JOIN v2_datasets d ON s.dataset_id = d.id
WHERE d.created_at > NOW() - INTERVAL '24 hours';
``` 

## üß† Improvements & Additional Data to Capture

- Persist host metrics snapshots per dataset/time window in DB (new table proposal, e.g., `v2_host_metrics`):
  - dataset_id, t_start, t_end, avg_cpu_user, avg_cpu_system, avg_cpu_iowait, avg_mem_used_pct,
    avg_swap_used_mb, avg_load1, avg_load5, avg_load15
- Capture container-level stats and exit reasons:
  - docker exit code, last N lines of stdout/stderr, container OOM (from cgroup/journal), image tag
- Log processing parameters for reproducibility:
  - ODM command/flags (resolution, feature quality, neighbors), environment (DEV/PROD)
- Enrich raw data inventory:
  - image_count (done), total_size_bytes (done), per-extension counts, size histogram, skipped/corrupt files list
- EXIF summary:
  - first/last `DateTimeOriginal`, camera `Make/Model`, focal length range, GPS availability ratio
- Stage timing breakdown:
  - extract zip, RTK detection, EXIF extraction, ODM stages (opensfm, mve/meshing, orthophoto), COG, thumbnail
- Storage/IO telemetry:
  - SSH/S3 transfer durations, retries, bandwidth; disk free space checks before processing
- Error taxonomy & remediation hints:
  - classify frequent failures (CRS missing, corrupt TIFF, projection mismatch, insufficient overlap) and suggest fixes

Operational tips to speed triage:
- Always derive the exact processing window from `v2_logs` and correlate with sysstat/journalctl.
- Keep small helper scripts to map dataset timestamp ‚Üí `saDD` file and run `sar` commands automatically.
- Consider writing a CLI helper that outputs a compact resource summary next to DB log excerpts for any dataset.