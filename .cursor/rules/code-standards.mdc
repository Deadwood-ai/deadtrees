---
description: Essential programming standards, API patterns, and error handling
globs: 
alwaysApply: false
---
# Code Standards & Best Practices

## ðŸ“‹ **WHEN TO USE THIS RULE**
**Agent should request this rule when:**
- Writing or reviewing Python code
- Building FastAPI endpoints or services
- Implementing error handling and logging
- Working with data models or validation
- Adding new metadata types or geospatial processing

## ðŸŽ¯ **CORE PRINCIPLES**
- **Functional Programming**: Prefer functions over classes where possible
- **Type Safety**: Use Pydantic models and comprehensive type hints
- **Error-First Design**: Handle errors at function entry, use early returns
- **RORO Pattern**: Receive an Object, Return an Object
- **Async-First**: Use async/await for I/O operations, sync for CPU-bound
- **Real Data Testing**: Use real datasets and fixtures, avoid mocking where possible

## ðŸ“ **PYTHON STANDARDS**

### Code Style
```python
# PEP 8 with project settings: max line length 120, single quotes, tabs
from typing import Optional, List, Dict, Any
from pydantic import BaseModel

def process_dataset(dataset_id: int, options: Optional[Dict[str, Any]] = None) -> ProcessingResult:
    """Process dataset with optional configuration."""
    # Error handling first
    if dataset_id <= 0:
        raise ValueError("Dataset ID must be positive")
    
    if not dataset_exists(dataset_id):
        raise DatasetNotFoundError(f"Dataset {dataset_id} not found")
    
    # Happy path
    return perform_processing(dataset_id, options or {})
```

### Type Hints & Validation
```python
from pydantic import BaseModel, Field
from typing import Optional, List

class DatasetRequest(BaseModel):
    file_name: str = Field(..., min_length=1)
    user_id: str = Field(..., description="User UUID")
    metadata: Optional[Dict[str, Any]] = None

class ProcessingResponse(BaseModel):
    success: bool
    dataset_id: Optional[int] = None
    error_message: Optional[str] = None
```

### Error Handling Pattern
```python
# Custom exception hierarchy
class DeadTreesError(Exception):
    """Base exception for DeadTrees operations"""
    pass

class DatasetNotFoundError(DeadTreesError):
    """Dataset does not exist"""
    pass

class ProcessingError(DeadTreesError):
    """Processing operation failed"""
    pass

# Error-first function design
def process_file(file_path: str) -> ProcessingResult:
    # Validate inputs first
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")
    
    if not file_path.endswith(('.tif', '.tiff')):
        raise ValueError("Only TIFF files are supported")
    
    # Process with structured error handling
    try:
        result = perform_processing(file_path)
        return ProcessingResult(success=True, result=result)
    except Exception as e:
        logger.error(f"Processing failed for {file_path}: {e}")
        raise ProcessingError(f"Failed to process {file_path}") from e
```



### Dataset Access Pattern
```python
import xarray as xr
from pathlib import Path

def get_dataset_path() -> Path:
    """Get dataset path with existence validation."""
    dataset_path = settings.base_path / "assets" / "data" / "dataset.zarr"
    if not dataset_path.exists():
        raise FileNotFoundError(f'Dataset not found at {dataset_path}')
    return dataset_path

```

## ðŸ“Š **METADATA EXTENSION PATTERN**

### Adding New Metadata Types
```python
# 1. Add to MetadataType enum
class MetadataType(str, Enum):
    GADM = 'gadm'
    BIOME = 'biome'
    PHENOLOGY = 'phenology'  # New type
    
# 2. Create metadata model with validation
class NewMetadata(BaseModel):
    """Structure for new metadata type"""
    data_field: List[int]
    source: str = 'Data Source'
    version: str = '1.0'
    
    @field_validator('data_field')
    @classmethod
    def validate_data(cls, v: List[int]) -> List[int]:
        """Custom validation logic"""
        if not v or len(v) != EXPECTED_LENGTH:
            raise ValueError("Data must have expected length")
        return v

# 3. Create utility functions
def get_new_metadata(lat: float, lon: float) -> Optional[NewMetadata]:
    """Get new metadata for location."""
    data = get_data_for_location(lat, lon)
    return NewMetadata(data_field=data) if data else None

# 4. Integrate into metadata processing
def process_metadata(task: QueueTask, temp_dir: Path):
    # ... existing processing ...
    
    # Add new metadata type
    new_metadata = get_new_metadata(lat=bbox_centroid[1], lon=bbox_centroid[0])
    
    metadata_dict = {
        MetadataType.GADM: admin_metadata.model_dump(),
        MetadataType.BIOME: biome_metadata.model_dump()
    }
    
    # Conditional inclusion
    if new_metadata:
        metadata_dict[MetadataType.NEW_TYPE] = new_metadata.model_dump()
```

## ðŸ§ª **TESTING PATTERNS**

### Real Data Testing (Preferred over Mocking)
```python
# Use real datasets and coordinates
TEST_POINTS_WITH_DATA = [
    (48.0, 8.0, "Black Forest, Germany"),
    (45.0, -75.0, "Eastern Canada"),
    (35.0, -120.0, "California"),
]

TEST_POINTS_NO_DATA = [
    (30.0, -30.0, "Atlantic Ocean"),
    (0.0, -150.0, "Pacific Ocean"),
]

@pytest.mark.parametrize('lat,lon,location', TEST_POINTS_WITH_DATA)
def test_data_retrieval_with_real_coordinates(lat, lon, location):
    """Test data retrieval for real geographic locations."""
    result = get_data_for_location(lat, lon)
    
    if result is not None:
        assert isinstance(result, list)
        assert len(result) == EXPECTED_LENGTH
        assert all(isinstance(val, int) for val in result)
        assert all(0 <= val <= MAX_VALUE for val in result)
        
@pytest.mark.parametrize('lat,lon,location', TEST_POINTS_NO_DATA)  
def test_data_retrieval_no_data_locations(lat, lon, location):
    """Test graceful handling of locations without data."""
    result = get_data_for_location(lat, lon)
    assert result is None, f"Should return None for {location}"
```

### Integration Testing Pattern
```python
def test_metadata_processing_integration(metadata_task, auth_token):
    """Test that new metadata is properly integrated."""
    # Process metadata
    process_metadata(metadata_task, settings.processing_path)
    
    # Verify database storage
    with use_client(auth_token) as client:
        response = client.table(settings.metadata_table).select('*').eq('dataset_id', metadata_task.dataset_id).execute()
        
    metadata = response.data[0]['metadata']
    
    # Check new metadata type (conditional)
    if MetadataType.NEW_TYPE in metadata:
        new_data = metadata[MetadataType.NEW_TYPE]
        assert 'data_field' in new_data
        assert len(new_data['data_field']) == EXPECTED_LENGTH
        assert new_data['source'] == 'Expected Source'
```

## ðŸš€ **FASTAPI PATTERNS**

### Endpoint Structure
```python
from fastapi import FastAPI, HTTPException, Depends
from pydantic import BaseModel

app = FastAPI()

@app.post("/datasets", response_model=DatasetResponse)
async def create_dataset(request: DatasetRequest) -> DatasetResponse:
    """Create new dataset with validation."""
    # Input validation (automatic via Pydantic)
    # Additional validation if needed
    if not await user_exists(request.user_id):
        raise HTTPException(status_code=400, detail="Invalid user ID")
    
    # Business logic
    try:
        dataset = await create_dataset_record(request)
        return DatasetResponse(success=True, dataset_id=dataset.id)
    except Exception as e:
        logger.error(f"Dataset creation failed: {e}")
        raise HTTPException(status_code=500, detail="Dataset creation failed")
```

### Async/Sync Patterns
```python
# Use async for I/O-bound operations
async def fetch_dataset(dataset_id: int) -> Dataset:
    """Fetch dataset from database."""
    async with get_db_client() as client:
        result = await client.table('v2_datasets').select('*').eq('id', dataset_id).execute()
        return Dataset(**result.data[0]) if result.data else None

# Use sync for CPU-bound operations  
def process_image(image_data: bytes) -> ProcessedImage:
    """Process image data synchronously."""
    # CPU-intensive image processing
    return perform_image_processing(image_data)
```

### Error Response Structure
```python
# Consistent error responses
@app.exception_handler(DeadTreesError)
async def deadtrees_error_handler(request: Request, exc: DeadTreesError):
    return JSONResponse(
        status_code=400,
        content={
            "error": {
                "code": exc.__class__.__name__,
                "message": str(exc)
            }
        }
    )
```

## ðŸ“Š **STRUCTURED LOGGING**

### Logging Pattern
```python
from shared.logging import UnifiedLogger, LogContext, LogCategory

logger = UnifiedLogger()

def process_dataset(dataset_id: int, user_id: str) -> ProcessingResult:
    context = LogContext(
        category=LogCategory.PROCESS,
        dataset_id=dataset_id,
        user_id=user_id
    )
    
    logger.info("Processing started", context=context)
    
    try:
        result = perform_processing(dataset_id)
        logger.info("Processing completed", context=context.with_extra({
            'processing_time': result.processing_time,
            'output_size': result.output_size
        }))
        return result
    except Exception as e:
        logger.error("Processing failed", context=context.with_extra({
            'error_type': type(e).__name__,
            'error_details': str(e)
        }))
        raise
```

### Log Categories
```python
from enum import Enum

class LogCategory(Enum):
    UPLOAD = "upload"
    PROCESS = "process"
    COG = "cog"
    THUMBNAIL = "thumbnail"
    METADATA = "metadata"
    DEADWOOD = "deadwood"
```

## ðŸ”„ **RESOURCE MANAGEMENT**

### Context Managers
```python
from contextlib import contextmanager
import tempfile
import shutil

@contextmanager
def temporary_directory():
    """Context manager for temporary directory cleanup."""
    temp_dir = tempfile.mkdtemp()
    try:
        yield temp_dir
    finally:
        shutil.rmtree(temp_dir)

# Database connections
@contextmanager
async def database_transaction():
    """Async context manager for database transactions."""
    async with get_db_client() as client:
        transaction = await client.begin()
        try:
            yield client
            await transaction.commit()
        except Exception:
            await transaction.rollback()
            raise
```

### Memory Management
```python
def process_large_file_in_chunks(file_path: str, chunk_size: int = 8192) -> Iterator[bytes]:
    """Process large files in memory-efficient chunks."""
    with open(file_path, 'rb') as f:
        while chunk := f.read(chunk_size):
            yield chunk
```

## ðŸ“š **CONFIGURATION MANAGEMENT**

### Settings Pattern
```python
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    # Database
    supabase_url: str
    supabase_key: str
    
    # Processing  
    max_concurrent_tasks: int = 2
    processing_timeout: int = 3600
    
    # Development
    debug_mode: bool = False
    log_level: str = "INFO"
    
    # Optional
    logfire_token: Optional[str] = None
    
    class Config:
        env_file = ".env"
        case_sensitive = False

# Global settings instance
settings = Settings()
```

## ðŸ“‹ **DOCUMENTATION STANDARDS**

### Docstring Format
```python
def process_geospatial_data(
    input_path: str, 
    output_path: str, 
    options: Optional[ProcessingOptions] = None
) -> ProcessingResult:
    """Process geospatial data with specified options.
    
    Args:
        input_path: Path to input geospatial file
        output_path: Path where processed file will be saved
        options: Optional processing configuration
        
    Returns:
        ProcessingResult containing operation details and metrics
        
    Raises:
        FileNotFoundError: If input file doesn't exist
        ValidationError: If input file format is invalid
        ProcessingError: If processing operation fails
    """
    pass
```

## ðŸŽ¯ **BEST PRACTICES SUMMARY**

### Code Quality
1. **Use comprehensive type hints** for all function signatures
2. **Implement error-first design** with early returns
3. **Use Pydantic models** for data validation and configuration
4. **Follow PEP 8** with project-specific formatting rules
5. **Write descriptive docstrings** with examples and error documentation

### Geospatial & Scientific Data
1. **Use real data testing** instead of mocking for geospatial functions
2. **Handle coordinate transformations** explicitly with proper CRS definitions
3. **Implement graceful degradation** for optional data sources
4. **Use parametrized tests** with real geographic coordinates
5. **Validate data length and ranges** for scientific datasets

### Metadata Extension
1. **Follow the 4-step pattern** for adding new metadata types
2. **Use conditional inclusion** for optional metadata
3. **Implement proper validation** in Pydantic models
4. **Test integration** with the full metadata processing pipeline

### Performance  
1. **Use async/await** for I/O-bound operations
2. **Process large files in chunks** to manage memory
3. **Use context managers** for resource cleanup
4. **Implement proper connection pooling** for external services

### Error Handling
1. **Create custom exception hierarchies** for domain-specific errors
2. **Use structured logging** with LogContext for all operations
3. **Distinguish between temporary and permanent errors**
4. **Handle resource cleanup** properly in error scenarios
